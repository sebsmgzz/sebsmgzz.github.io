[
    {
        "name": "Whatsapp ChatBot with ChatGPT PoC",
        "useCase": "The use case was to provide customer service assitance in an automated mechanism. This increases the overall business experience for the end user and increased the response times as well. Key benefits for the business included automated and 24h avaialble customer support.",
        "description": "The project consisted in developing a ChatBot as a Node.js web application that was designed as a background worker communicating with WhatsApp's servers. The application was hosted as WebApp in Azure to display a QR code through which one could connect a phone, however there was no need for the end user to reach out to it. Under the hood, the web application received the messages from the user and generated a matching response using ChatGPT conversations. It required tokeep a cached database of each converation history. The application required a ChatGPT api key that was preconfigured beforehand, and it used the framework BotWhatsApp for the WhatsApp delegation. The bot was linked with the Baileys provider. Initially we started with separated applications, one to anwser pre-configured WhatsApp responses, and another one that was a simple chat room with ChatGPT, eventually merging them both into a single application. The team developed jupiter notebooks to be used as reference for the ChatGPT SDK, but eventually had to bee rewriten to JS using Node.js.",
        "date": "2023-05-08",
        "stack": [ "node", "js", "python", "azure", "openai" ]
    },
    {
        "name": "Microservices enabled for messaging",
        "useCase": "In a microservice architecture, the microservices need to communicate between themselves. In order to enable this, we needed to create a messaging system for microservices to produce and consume messages on demand. Additionally, the deployment of the microservices neededs to be done automatically by leveraging infrastructure as code.",
        "description": "The project and split in two parts, the development of the microservices and the communication among them, and the CI/CD part of it that used IaC to deploy the microservices. Since we where dealing with a small amount of microservices we keept all of them in a single GitHub repostiory. For each microservice we developed the Dockerfile and all of them where written in C# .Net Core. Initially, and for the development environment, the microservices used rabbitMQ for the messaging. This allowed developers to host their own instance of the containers by using docker compose, and RabbitMQ was the message broker among services. In production however, we used Azure Service Bus to act as a message broker. The configuration of this infrastructure was stored in the same repository in bicep files, and it was triggered by GitHub workflows (Actions). The workflow consisted of three jobs: 1. Build the required infrastructure including an Azure Container Registry (ACR). 2. Build the container image for each microservice and publish it to the ACR. 3. Create or update the microservice as an Azure Container App.",
        "date": "2023-03-13",
        "stack": [ "csharp", "dotnetCore", "docker", "kubernetes", "azure", "rabbitMq", "github" ]
    },
    {
        "name": "Optical characters translations API",
        "useCase": "The project was meant to be used to translate PDF files between multiple languages, but primarly and initially from english to spanish. The API was meant to be consumed by RPA clients in order to automate the invoice processing. Since some invoices had been created in english, they needed to be translated to spanish in order to provide the proper automation flow at the client side.",
        "description": "The project was a .NET Core API in C# that provided endpoints to read and translate the invoices, and it was developed using a domain drive design approach. The flow was as follows: The RPA client uploads the PDF to a storage account, then calls an endpoint in the API that will receive the URL to that PDF file. Internally the API stores a reference to that document and starts reading it asyncronously using Azure Cognitive Services (Computer vision). The client then calls the translation endpoint to retrieve the document, so the API reads the document characters form the cognitive service and then calls the translations cognitive service to translate the text and sends it back to the client, already translated. Note that while the use case was for PDF files, and english to spanish translations, the API supported other file formats such as JPG images, as well as other languages translations which needed to be specified in the URL query parameters. The spanish language was just the default queray paramter value.",
        "date": "2023-01-30",
        "stack": [ "csharp", "dotnetCore", "azure", "java" ]
    },
    {
        "name": "Facial scan and identity verification API",
        "useCase": "The use case for this project was to be able to recognize if the person using the application, was in fact, the person they claimed to be. This was done using official government issued credentials a reference, and a scan of the user's face as evidence. The users will take a picture of their credential as well as a scan of their face in order to verify they are the same person in the credential.",
        "description": "The API leveraged the Azure Cognitive Services (Facial recognition) resource in order to compare faces. The user will take a picture of their credential as well as a short video of their face from left to right. The video will be split into images and that whole set of images will be used to compare against the image from the credential, hence different angles of the same face are been compared. Once those images where sent, the application will respond with a single value indicating if the user is considered the same as the one in the credential. Under the hood, the application implemented business logic such as 'Images with under X% percent of certainity are considered invalid' and 'All images must be valid for us to consider the user the same as the one in the credential'.",
        "date": "2022-10-03",
        "stack": [ "csharp", "dotnetCore", "xamarin", "azure" ]
    },
    {
        "name": "Microservices and machine learning for anomaly detection",
        "useCase": "In the manufacturing industry, machines need to undergo service maintenance. The simple usage of the machinery weares them out, reducing efficiency and increasing costs, hence the importance of maintenance. Due to the huge amount of variables to take into consideration it is extremely difficult to determine when and where a machine will fail. However, while failure of the machine might happen instanteneously, the wear off of it, is not. We can detect if a machine's efficiency is changing over time, and based off its historical data, determine of there is the presence of an anomaly with highly probability that will cause the machine to fail in the nearby future. By knowing this before hand, we can make an exploratory analysis in the machinery to try detect and fix the root of the anomaly.",
        "description": "The project consisted of a combination of IoT devices pulling data from the machinery and sending it as telemetry to the Azure cloud. We developed the internal container images that the devices must leverage to read and push the data to the correct routes. In Azure, the data was centralized in Azure Data Explorer, for the machine learning services to consume and train whenever needed, as well as to trigger the existing ML model, which in turn will send a message to a Service Bus whenever an anomaly is detected and trigger an email alert to notify the proper personel.",
        "date": "2022-04-04",
        "stack": [ "csharp", "python", "dotnetCore", "azure", "cosmos" ]
    },
    {
        "name": "Energy demand response system",
        "useCase": "Everything needs electricity nowdays, and electricity comes, directly or indirectly from the electric grid. Due to the nature of electricity, the cost of it varies from time to time. Given the case of smart batteries, it would be ideal that these are charged when the cost is the lowest, and used when the cost is the highest. To achieve this 'demand response', we had to develop a subscription system for which each battery subscribes to events provided by, either the energy provider or the battery owner.",
        "description": "To develop this project, we had to develop embedded C software in the smart batteries that provide the telemetry related to their data such as charge level. The telemetry data was sent to GCP for storage, and then sent to Azure Event Hubs. From there, the api services consumed the data to determine which batteries are able to be enabled to start the charging, and which ones cannot.",
        "date": "2021-12-06",
        "stack": [ "gcp", "azure", "cpp", "dotnetCore", "cosmos", "react" ]
    },
    {
        "name": "IoT medical devices and telemetry",
        "useCase": "The medical industry requires high standards in data protection and privacy, in the case of smart connected devices, those standards are even higher and so are the challenges. However, through the Azure cloud we where able to provide a highly secure telemetry system for IoT medical devices for real time data analysis. With the system in place, doctors and medical staff could read real time data from their own device or through a web portal as well as to have access to real time analytics of the telemetry provided. This included charts, graphs, statistics and more information about both, their patients and the devices owned by the hospital.",
        "description": "The project consisted on the development of three main layers: backend apis and services in the cloud in C# .Net core, frontend dashboards in powerbi and react, and edge applications running as docker containers in the IoT devices. We leveraged the Azure IoT Hub resource in order to manage devices. The development of the edge modules ended in a container image in an Azure Container Registry (ACR) which the IoT devices used to pull images. The edge containers ended up running the business logic that created the telemetry, which was pushed into the IoT Hub. From the IoT Hub, there where multiple areas where the telemetry could end, some ended stored in storage accounts, other in cosmos, etc. All this telemetry routing was done with the help of the Event Hubs and Stream Analytics Azure resources. Here a key architectural pattern refering to the hot, warm and cold paths came into play to determine the best route for the telemetry, as well as to the data sent back to the devices. Finally, with the data in the cloud, a simple UI application was developed in order to view the data and its analysis as well as to manage the devices. The data was displayed in two forms, one been a React SPA and the other been PowerBI charts and graphics.",
        "date": "2021-01-03",
        "stack": [ "csharp", "dotnetCore", "docker", "azure", "cosmos", "powerbi", "js", "node", "react" ]
    }
]
